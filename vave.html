<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initialscale=1" />
    <title>Ameya Nikose</title>
    <link rel="icon" type="image/png" href="homepage/Icon.png" />
    <link
      href="https://fonts.googleapis.com/css2?family=Manrope:wght@200;300;400;500;600;700;800&display=swap"
      rel="stylesheet"
    />
    <link
      href="https://fonts.googleapis.com/css2?family=Lora:ital,wght@0,400;0,500;0,600;0,700;1,400;1,500;1,600;1,700&display=swap"
      rel="stylesheet"
    />
    <link
      href="https://fonts.googleapis.com/css2?family=Literata:ital,wght@0,200;0,300;0,400;0,500;0,600;0,700;0,800;0,900;1,200;1,300;1,400;1,500;1,600;1,700;1,800;1,900&display=swap"
      rel="stylesheet"
    />

    <link rel="stylesheet" type="text/css" href="Style2.css" />
    <link rel="stylesheet" href="https://unpkg.com/aos@next/dist/aos.css" />
  </head>
  <body>
    <!-- Introduction -->
    <main>
      <header>
        <div class="logo">
          <a class="logo" href="index.html">Ameya Nikose</a>
        </div>
        <nav>
          <ul class="nav-links">
            <!-- <li><a class="nav-link" href="#about">About</a></li> -->
            <li>
              <a class="nav-link" href="https://ameyanikose.github.io/#work"
                >Work</a
              >
            </li>
            <li>
              <a class="nav-link" href="Resume.pdf" target="_blank">Resume</a>
            </li>
          </ul>
        </nav>
      </header>
    </main>

    <img class="title-image" src="homepage/vave isometric.jpg" alt="Vave" />

    <!-- Title -->
    <div
      class="title"
      data-aos="fade"
      data-aos-duration="2200"
      data-aos-once="true"
    >
      <p>Vave</p>
      <div class="sub-title">
        A gesture-based, intuitive musical instrument with immersive
        audio-reactive visuals.
      </div>
      <div class="detail">
        January, 2020 • 3 Weeks • Interaction Design • Product Design
        <br />
        In Collaboration with
        <a href="https://harsuyash.github.io/" target="_blank">Suyash Sinha.</a>
      </div>
    </div>

    <!-- Writeup -->

    <div
      id="myBtn"
      onclick="topFunction()"
      title="Go to top"
      data-aos="fade"
      data-aos-duration="2200"
      data-aos-once="true"
    >
      <img
        src="homepage/arrow2.png"
        alt="Scroll Up"
        width="40px"
        height="40px"
      />
    </div>

    <!-- <div
      class="body"
      data-aos="fade"
      data-aos-duration="2200"
      data-aos-once="true"
    >
      <h1>Work In Progress, <br />Check back soon later. :)</h1>
    </div> -->

    <div
      class="body"
      data-aos="fade"
      data-aos-duration="2200"
      data-aos-once="true"
    >
      <h1>Abstract</h1>
      <p>
        Vave, a 3-week long project done in collaboration with Suyash Sinha, is
        an exploration of mixed media interaction techniques by creating a
        virtual gesture-based instrument. The project’s motivation was to learn
        to work with augmented/virtual/mixed reality and create an enjoyable
        musical experience.
      </p>
      <br />
      <h1>Vision</h1>
      <p>
        To create an enjoyable musical experience while exploring interactions
        in augmented/virtual reality.
      </p>
      <br />
      <h1>Ultimate Concept</h1>
      <p>
        The final product was a handy instrument played by waving and making
        other hand gestures over it. The physical instrument was to have a
        tangible mode of interaction. It was finally interfaced using Unity with
        visualizations that could be seen in VR for visual feedback.
      </p>
    </div>

    <div data-aos="fade" data-aos-duration="2200" data-aos-once="true">
      <img class="image2" src="vave/vave side view.jpg" />
      <p class="description"><i>The Instrument</i></p>
    </div>

    <div
      class="body"
      data-aos="fade"
      data-aos-duration="2200"
      data-aos-once="true"
    >
      <h1>Initial Ideation and Concept</h1>
      <p>
        To create an enjoyable music experience, we started by learning about
        recent technologically driven musical instruments, games, and
        experiences. The first challenge that we faced was to define whether we
        were going to create an instrument or a game. We looked at Beat-Saber (
        an existing musical game in virtual reality). Initially, we came up with
        some ideas on whether to carry forward with creating an instrument or a
        game.<br />
        After speculating a bit about the idea of an instrument, we selected the
        concept of creating an instrument.
      </p>
    </div>

    <div data-aos="fade" data-aos-duration="2200" data-aos-once="true">
      <img class="image2" src="vave/ideation.png" />
      <p class="description">
        <i>Initial Ideas</i>
      </p>
    </div>

    <div
      class="body"
      data-aos="fade"
      data-aos-duration="2200"
      data-aos-once="true"
    >
      <h1>Inspirations for creating an Instrument</h1>
      <p>
        To create an enjoyable experience, we again dived deeper and looked for
        existing technical music instruments and bodily experiences to create
        music. The inspirations for digital musical instruments were of coretet
        and theremin.
      </p>
      <br />
      <br />
      <h2>Coretet</h2>
      <br />
      <p>
        Coretet is a set of dynamic gesture-driven, musical instruments for
        virtual reality performances. They have created virtual string and
        percussion instruments like cello, violin, and an orb that can be played
        using a virtual reality headset and controllers.
        <br />
        <br />
      </p>

      <div data-aos="fade" data-aos-duration="2200" data-aos-once="true">
        <iframe
          width="100%"
          height="400vh"
          src="https://www.youtube.com/embed/f6ABPxqu0Is"
          frameborder="0"
          allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
          allowfullscreen
        ></iframe>
      </div>
      <br />
      <br />
      <h2>Theremin</h2>
      <br />
      <p>
        The theremin is an electronic musical instrument controlled without
        physical contact by the thereminist (performer). The instrument's
        controlling section usually consists of two metal antennas that sense
        the relative position of the thereminist's hands and control oscillators
        for frequency with one hand and amplitude (volume) with the other. The
        electric signals from the theremin are amplified and sent to a
        loudspeaker.
      </p>
      <br />

      <div data-aos="fade" data-aos-duration="2200" data-aos-once="true">
        <iframe
          width="100%"
          height="400vh"
          src="https://www.youtube.com/embed/LYSGTkNtazo"
          frameborder="0"
          allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
          allowfullscreen
        ></iframe>
      </div>
      <br />
      <br />
      <h2>Musical Instrument in AR</h2>
      <br />
      <p>
        The video from Yago De Quay about Augmented Reality Musical Instrument
        has a very interesting concept of controlling the song/music parameters
        by hand movements using motion capture technology.
      </p>

      <br />
      <div data-aos="fade" data-aos-duration="2200" data-aos-once="true">
        <iframe
          width="100%"
          height="400vh"
          src="https://www.youtube.com/embed/11glZ3U1Txs"
          frameborder="0"
          allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
          allowfullscreen
        ></iframe>
      </div>
      <br />
      <br />
      <h2>Musical Installation</h2>
      <br />
      <p>
        We also looked at different experiences like piano-tap-dance, where the
        players played a huge piano by dancing on the notes to play it.
      </p>
      <br />
      <div data-aos="fade" data-aos-duration="2200" data-aos-once="true">
        <iframe
          width="100%"
          height="400vh"
          src="https://www.youtube.com/embed/PK-Tn6R8f0s"
          frameborder="0"
          allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
          allowfullscreen
        ></iframe>
      </div>
      <br />
      <br />
      <h2>Mr. Bean- Gesture and Emotion Driven</h2>
      <br />
      <p>
        Finally, we had an idea of creating a musical instrument that is solely
        gesture and emotion-driven. This can be explained by this video of Mr.
        Bean, where he uses emotions to drive and conduct the choir. Wouldn’t it
        be awesome to play an instrument like that someday!
      </p>
      <br />
      <div data-aos="fade" data-aos-duration="2200" data-aos-once="true">
        <iframe
          src="https://drive.google.com/file/d/1w8TM8_G46tEr1aqpKY5HIl5DFPPCmF4H/preview"
          width="100%"
          height="400vh"
          frameborder="0"
          allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
          allowfullscreen
        ></iframe>
      </div>
      <br />
      <br />
      <h1>Explorations for Creating an Instrument</h1>
      <p>
        After getting inspired by looking at some wild things on the internet,
        we decided to also look into the technical part of the process and
        explore various media. So we started exploring different ways of
        building the instrument simultaneously. We had a lot of fun trying to
        learn about and quickly prototype different interaction techniques in
        AR/VR/MR. As we did not have access to existing controllers, we had to
        look at other alternatives.<br /><br />
      </p>
      <br />
      <h2>Figuring Out Ways to Interact</h2>
      <br />
      We looked at methods to interact without a controller, like using a
      raycast reticle to point and shoot using a phone, using virtual buttons on
      an image marker in AR, and using onscreen buttons in AR. <br /><br />
      <br />
      <h2>VR Gaze</h2>
      <br />
      One simple way to interact in VR was by using the accelerometer of the
      phone and the Raycast Reticle feature in Google VR SDK that tracks where
      the person is looking/ gazing and triggers a pre-defined interaction with
      that element. Here in our prototype, when a person gazes at a cube, the
      cube changes its color from white to red and if the person continues to
      stare for a specified time limit, it changes to black.

      <br />
    </div>

    <div data-aos="fade" data-aos-duration="2200" data-aos-once="true">
      <img class="image2" src="vave/vr gaze.png" />
      <p class="description">
        <i
          >VR Gaze with Raycast Reticle : A simple inbuilt interaction but
          moving around a lot to point the phone at elements in space could be
          tiring.
        </i>
      </p>
    </div>

    <div
      class="body"
      data-aos="fade"
      data-aos-duration="2200"
      data-aos-once="true"
    >
      <br />
      <h2>On-screen buttons in AR</h2>
      <br />
      <p>
        In our next prototype, some keys pop up in the AR space and when these
        are pressed on the screen, a song is played. The learning from this
        exercise was; although in AR, the experience was like any other 2D
        application and not enjoyable.
      </p>
      <br />
      <br />

      <div data-aos="fade" data-aos-duration="2200" data-aos-once="true">
        <iframe
          src="https://drive.google.com/file/d/1I2K7bHW_BNl8JiZsv2dgK-48JHLsNw9o/preview"
          width="100%"
          height="400vh"
          frameborder="0"
          allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
          allowfullscreen
        ></iframe>
      </div>
    </div>
    <div
      class="body"
      data-aos="fade"
      data-aos-duration="2200"
      data-aos-once="true"
    >
      <br />
      <h2>Mixed-Reality and Virtual Buttons</h2>
      <br />
      <p>
        We also prototyped wearable AR by making a stereoscopic AR camera in
        Unity. This helped in having a tracker-based AR with virtual buttons, at
        the same time keeping your hands free, unlike hand-held AR using a
        mobile device. As the hands were free, we could add some virtual buttons
        in the 3D world space that could be interacted with using our hands. The
        learning from this exercise was; interactions became more immersive but
        the virtual buttons only work when visible through the camera.
      </p>
      <br />
    </div>

    <div
      class="body"
      data-aos="fade"
      data-aos-duration="2200"
      data-aos-once="true"
    >
      <div data-aos="fade" data-aos-duration="2200" data-aos-once="true">
        <iframe
          src="https://drive.google.com/file/d/1Hhr9VtV-WVZbDr6d5KZX1w6ri7e1fQdN/preview"
          width="100%"
          height="400vh"
          frameborder="0"
          allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
          allowfullscreen
        ></iframe>
      </div>
      <br />
      <br />
      <h2>Making Our Own Controllers</h2>
      <br />
      <p>
        We also tried to make our own controllers with simple sensors like
        accelerometer, gyroscope, and ultrasonic sensor and map hand movements
        recorded through them to different musical variables. The learning from
        this exercise was; as the sensors were independent from AR camera, one
        could look anywhere while interacting with the controllers. This also
        enabled a more tangible mode of interacting and we decided to go further
        with this.
      </p>
      <br />
      <br />
      <div data-aos="fade" data-aos-duration="2200" data-aos-once="true">
        <iframe
          src="https://drive.google.com/file/d/1YtswqbRBc06HIv_NWGx6rzUJRau01JQi/preview"
          width="100%"
          height="400vh"
          frameborder="0"
          allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
          allowfullscreen
        ></iframe>
      </div>
      <br />
      <br />
      <h2>Audio Helm, Using VST for Instrument</h2>
      <br />
      <p>
        Another challenge was to produce musical sounds in Unity based on the
        feedback from sensors/ virtual buttons/ raycast. We used the Audio Helm
        plugin, which is a live audio synthesizer, sequencer, and sampler for
        Unity that gives you the tools to create dynamic sound effects and
        generative music. We then tweaked the code to control audio helm
        variables according to the input from sensors to make it interactive
        with gestures.
      </p>
    </div>
    <div data-aos="fade" data-aos-duration="2200" data-aos-once="true">
      <img class="image2" src="vave/Audio Helm.png" />
      <p class="description">
        <i>Audio Helm VST</i>
      </p>
      <br />
      <br />
    </div>

    <div
      class="body"
      data-aos="fade"
      data-aos-duration="2200"
      data-aos-once="true"
    >
      <h2>Visualizations</h2>
      <br />
      <p>
        Simultaneously we also explored creating visualizations for the
        instrument that will be visible on the VR headset. One of the
        visualizations was a large number of instances of a sphere with a
        glowing material. The amount of glow, gravity, and scatter were linked
        according to different frequencies. These visualizations would later
        serve while creating an immersive experience for the player.
      </p>
      <br />
      <br />
      <div data-aos="fade" data-aos-duration="2200" data-aos-once="true">
        <iframe
          src="https://drive.google.com/file/d/1sdFPPp99m2NsvHfV6xxNfEE_9zJdgy_m/preview"
          width="100%"
          height="400vh"
          frameborder="0"
          allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
          allowfullscreen
        ></iframe>
      </div>
      <br />
      <br />
    </div>

    <div
      class="body"
      data-aos="fade"
      data-aos-duration="2200"
      data-aos-once="true"
    >
      <h1>Narrowing down and finding the best way to interact.</h1>
      <h2>Gestures for Playing Music</h2>
      <br />
      <p>
        For a holistic seamless experience, we not only had to figure out the
        technical aspects but also think about the usability and experience
        design aspects. In order to decide on the gestures that will be
        intuitive and easy to perform to control the music, we did some role
        play and act-it-out by simply doing actions that pretend to control
        existing music. These wizard of oz prototypes helped understand the
        different hand movements that are enjoyable and natural for controlling
        different music variables like pitch, volume, sustain, vibrato. We also
        took inspiration from the Mr. Bean video.
      </p>
      <br />
    </div>
    <div data-aos="fade" data-aos-duration="2200" data-aos-once="true">
      <img class="image2" src="vave/how to play.jpg" />
      <p class="description">
        <i>How to Play: Using gestures to control parameters.</i>
      </p>
      <br />
      <br />
    </div>
    <br />

    <div
      class="body"
      data-aos="fade"
      data-aos-duration="2200"
      data-aos-once="true"
    >
      <h2>Implementation - Wearable</h2>
      <br />
      <p>
        For connecting the intended design with the technology, we had to figure
        out the implementation for the different gestures. As most of the
        gestures were free-flowing hand gestures, we decided to go with a
        wearable glove with sensors. A 6 DOF accelerometer was used to get the
        rotational and transnational values of the hand movement. We added some
        modalities with buttons on the fingers. One could pinch with a thumb and
        index finger and move the hand as if holding and moving a virtual slider
        to control any variable.
      </p>
      <br />
    </div>

    <div data-aos="fade" data-aos-duration="2200" data-aos-once="true">
      <img class="image2" src="vave/wearable.jpg" />
      <p class="description">
        <i>Alternate Concept for a Wearable Instrument</i>
      </p>
      <br />
      <br />
    </div>
    <div
      class="body"
      data-aos="fade"
      data-aos-duration="2200"
      data-aos-once="true"
    >
      <h2>Discrete Vs Continuous Music</h2>
      <br />
      <p>
        One of the decisions that we had to make was whether to put a smooth
        transition between two semitones (like in a theremin) or to jump
        directly between semitones. Finally, after testing both the scenarios,
        we decided to put just the semitones as the notes between two semitones
        were difficult to play.
      </p>
      <br />
    </div>
    <div
      class="body"
      data-aos="fade"
      data-aos-duration="2200"
      data-aos-once="true"
    >
      <h1>Ultimate Concept</h1>

      <h2>Tangible Instrument - Vave Description, How to play</h2>
      <br />
      <p>
        For our concept so far, we got feedback on including something tangible
        to interact with. Having intimacy and interacting with another
        instrument makes the experience of playing music more enjoyable and also
        something that we are used to. Considering this, we designed a simple
        wave-like form that is coherent with the kind of interaction we want.
        It’s a light instrument that can be held with one arm like a child and
        played with by waving the other hand. It contains an ultrasonic sensor
        at one end, and its readings have been calibrated to play different
        notes of a scale on each crest and trough of the form. For the
        prototype, the instrument contains one octave of G Major scale.
      </p>
      <br />
    </div>

    <div data-aos="fade" data-aos-duration="2200" data-aos-once="true">
      <img class="image2" src="vave/top view.jpg" />
      <p class="description"></p>
      <br />
    </div>

    <div data-aos="fade" data-aos-duration="2200" data-aos-once="true">
      <img class="image2" src="vave/bottom view.jpg" />
      <p class="description"></p>
      <br />
    </div>
    <div data-aos="fade" data-aos-duration="2200" data-aos-once="true">
      <img class="image2" src="vave/front view.jpg" />
      <p class="description">
        <i>Top, Bottom and Front View of the Instrument. </i>
      </p>
      <br />
      <br />
    </div>

    <div data-aos="fade" data-aos-duration="2200" data-aos-once="true">
      <img class="image2" src="vave/Concept.jpg" />
      <p class="description">
        <i
          >Final Concept with headset and headphones for the full experience.
        </i>
      </p>
      <br />
      <br />
    </div>
    <div
      class="body"
      data-aos="fade"
      data-aos-duration="2200"
      data-aos-once="true"
    >
      <h2>Back end, sensor, Arduino, and interfacing through Unity</h2>
      <br />
      <p>
        The main component of the instrument was the ultrasonic sensor that
        measured the distance of the hand from one end of the instrument. This
        was recorded using an Arduino Uno, and the Arduino data was sent to
        Unity using serial port communication. In Unity, we used C# and Audio
        Helm to play a musical note based on the position of the hand. Another
        component was an accelerometer that recorded the angle of the hand to
        control the volume (flatten the hand on the instrument to lower volume
        and raise upright to increase.) The accelerometer also measured the
        vibrations of the hand at a position to play vibrato. We tried to
        optimize the code by setting an optimum range of the sensor and delays
        in sending data to Unity, so we don’t overwhelm unity with a lot of data
        creating a lag at the same time sending enough data to keep it precise
        to the hand movements. However, there was still some amount of lag with
        all the sensors and data coming in, so we decided to just go with the
        ultrasonic sensor for our MVP to play different notes with the hand
        movements.
      </p>
      <br />
    </div>
    <div data-aos="fade" data-aos-duration="2200" data-aos-once="true">
      <img class="image2" src="vave/tools 1.jpg" />
      <p class="description"></p>
      <br />
      <br />
    </div>
    <div data-aos="fade" data-aos-duration="2200" data-aos-once="true">
      <img class="image2" src="vave/tools 2.jpg" />
      <p class="description">
        <i>Tools used. </i>
      </p>
      <br />
      <br />
    </div>

    <div
      class="body"
      data-aos="fade"
      data-aos-duration="2200"
      data-aos-once="true"
    >
      <h2>Virtual Landscape, Collaborate</h2>
      <br />
      <p>
        We also needed some visual feedback for a complete experience. All this
        was interfaced using Unity itself. We used a sphere in the world space
        in VR that moves with the movement of the hand (the position of the
        sphere corresponds to the data from the ultrasonic sensor). On the path
        of the sphere, we laid some bars. When the sphere collides with a bar,
        it glows/ ripples, and a musical note corresponding to the bar is played
        using Audio Helm. For the working prototype, we kept it simple, but we
        also created richer visualizations that could enhance the experience of
        playing the instruments.<br />
        As the instrument is in VR, we decided to propose the possibility of a
        collaborative virtual environment where different people can come
        together for jamming. “Near far wherever you are, I believe in our VAVE,
        the show must go on and on…”
      </p>
      <br />
      <h2>Final Prototype</h2>
      <br />
      <div data-aos="fade" data-aos-duration="2200" data-aos-once="true">
        <iframe
          src="https://drive.google.com/file/d/1o-domfJymQX7QONgL3LkbqpvFSjNkkfZ/preview"
          width="100%"
          height="400vh"
          frameborder="0"
          allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
          allowfullscreen
        ></iframe>
      </div>
      <br />
      <h2>User Testing</h2>
      <br />
      <div data-aos="fade" data-aos-duration="2200" data-aos-once="true">
        <iframe
          src="https://drive.google.com/file/d/1JkTW5P6m0tLwn7ACo2gWyxlJwwhvbGH5/preview"
          width="100%"
          height="400vh"
          frameborder="0"
          allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
          allowfullscreen
        ></iframe>
        <br />
        <br />

        <h1>Would love feedbacks & suggestions, Thank You.</h1>
        <br />
      </div>
    </div>

    <!-- Other Projects -->

    <div
      class="body"
      data-aos="fade"
      data-aos-duration="2200"
      data-aos-once="true"
    >
      <h2>Other Projects</h2>
    </div>

    <div class="work" data-aos="fade" data-aos-duration="900">
      <a class="card-container VC" href="touchdesigner.html">
        <img class="card" src="homepage/touchdesigner.jpg" alt=" " />
        <p class="card-text">Touchdesigner Explorations</p>
        <p class="card-text-description">
          Created audio-reactive, code based music visualizations using
          Touchdesigner.
        </p>
      </a>

      <a class="card-container Info" href="turtle.html">
        <img class="card" src="homepage/turtle2.jpg" alt=" " />
        <p class="card-text">Turtle</p>
        <p class="card-text-description">
          An olympiad designed to promote value education amongst students of
          different age groups.
        </p>
      </a>
    </div>
    <div
      class="show-all"
      data-aos="fade"
      data-aos-duration="1200"
      data-aos-anchor-placement="top-bottom"
    >
      <a href="index.html#work">Show All </a>
    </div>

    <!-- Footer -->

    <footer>
      <div
        class="footer-text"
        data-aos="fade"
        data-aos-duration="1200"
        data-aos-once="true"
      >
        <p>
          Get in touch. <br />
          Find my resume <a href="Resume.pdf" target="_blank">here.</a>
        </p>
      </div>
      <div
        class="end-text"
        data-aos="fade"
        data-aos-duration="1200"
        data-aos-once="true"
        data-aos-anchor-placement="top-bottom"
      >
        <p>Designed with ♥︎ in Bhilai. &nbsp © 2020 Ameya Nikose.</p>
      </div>
    </footer>

    <!-- Script -->

    <script src="https://unpkg.com/aos@2.3.1/dist/aos.js"></script>
    <script src="app2.js"></script>
    <script>
      AOS.init();
    </script>
  </body>
</html>
